{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa Embedding Generation\n",
    "\n",
    "Generate semantic embeddings for all unique survey questions using locally cached RoBERTa-large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total question-survey pairs: 6,732\n",
      "Unique questions: 6,296\n",
      "Unique surveys: 45\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/processed/survey_questions_cleaned.csv')\n",
    "\n",
    "print(f\"Total question-survey pairs: {len(df):,}\")\n",
    "print(f\"Unique questions: {df['question_id'].nunique():,}\")\n",
    "print(f\"Unique surveys: {df['survey'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing unique questions...\n",
      "Questions to embed: 6,295\n",
      "\n",
      "Sample questions:\n",
      "   question_id                                      question_text\n",
      "0            1  At any time during this school year did you at...\n",
      "1            2  At any time during this school year did you re...\n",
      "2            3  At any time during this school year did you re...\n",
      "3            4  At any time during this school year did you at...\n",
      "4            5  Did you receive [online schooling or virtual l...\n",
      "5            6  For this next question, I’m going to read a li...\n",
      "6            7  How many different schools have you attended t...\n",
      "7            8  During the time you were homeschooled this sch...\n",
      "8            9                             What grade are you in?\n",
      "9           10  In what month did your current school year begin?\n"
     ]
    }
   ],
   "source": [
    "# Get unique questions to embed\n",
    "# Filter out any rows with NaN question_text\n",
    "print(\"\\nPreparing unique questions...\")\n",
    "unique_questions = df[['question_id', 'question_text']].dropna(subset=['question_text']).drop_duplicates('question_id').sort_values('question_id')\n",
    "\n",
    "# Ensure question_text is string type\n",
    "unique_questions['question_text'] = unique_questions['question_text'].astype(str)\n",
    "\n",
    "print(f\"Questions to embed: {len(unique_questions):,}\")\n",
    "print(f\"\\nSample questions:\")\n",
    "print(unique_questions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RoBERTa-large from ../models/roberta-large...\n",
      "  ✓ Tokenizer loaded\n",
      "  ✓ Model loaded\n",
      "  ✓ Model moved to cpu\n",
      "\n",
      "Model ready! (loaded in 0.2s)\n",
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer from local cache\n",
    "model_path = '../models/roberta-large'\n",
    "\n",
    "print(f\"Loading RoBERTa-large from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "print(\"  ✓ Tokenizer loaded\")\n",
    "\n",
    "model = RobertaModel.from_pretrained(model_path, local_files_only=True)\n",
    "print(\"  ✓ Model loaded\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"  ✓ Model moved to {device}\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"\\nModel ready! (loaded in {load_time:.1f}s)\")\n",
    "print(f\"Embedding dimension: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate RoBERTa embeddings for a list of texts.\n",
    "    Uses mean pooling of last hidden states.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Ensure all texts are strings\n",
    "    texts = [str(text) if text is not None else '' for text in texts]\n",
    "    \n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(0, len(texts), batch_size), \n",
    "                    desc=\"Generating embeddings\",\n",
    "                    total=num_batches,\n",
    "                    unit=\"batch\")\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Update progress bar with current batch info\n",
    "            current_q = min(i + batch_size, len(texts))\n",
    "            pbar.set_postfix({\"questions\": f\"{current_q}/{len(texts)}\"})\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Mean pooling\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden * attention_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "            # Move to CPU and convert to numpy\n",
    "            embeddings.append(mean_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EMBEDDING GENERATION\n",
      "======================================================================\n",
      "Total questions: 6,295\n",
      "Batch size: 32\n",
      "Device: cpu\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c8ecfb50d24fe499873136757add41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/197 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLETED!\n",
      "======================================================================\n",
      "Embeddings shape: (6295, 1024)\n",
      "Embedding dimension: 1024\n",
      "Total time: 6.4 minutes\n",
      "Average: 0.061 seconds per question\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all unique questions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EMBEDDING GENERATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total questions: {len(unique_questions):,}\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "question_texts = unique_questions['question_text'].tolist()\n",
    "embeddings = get_embeddings(question_texts, batch_size=32)\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPLETED!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"Total time: {embedding_time/60:.1f} minutes\")\n",
    "print(f\"Average: {embedding_time/len(embeddings):.3f} seconds per question\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding statistics:\n",
      "Mean: -0.0316\n",
      "Std: 0.9592\n",
      "Min: -31.3662\n",
      "Max: 2.3050\n",
      "\n",
      "Data quality checks:\n",
      "Any NaN values: False\n",
      "Any Inf values: False\n"
     ]
    }
   ],
   "source": [
    "# Basic sanity checks\n",
    "print(\"\\nEmbedding statistics:\")\n",
    "print(f\"Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"Std: {embeddings.std():.4f}\")\n",
    "print(f\"Min: {embeddings.min():.4f}\")\n",
    "print(f\"Max: {embeddings.max():.4f}\")\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"Any NaN values: {np.isnan(embeddings).any()}\")\n",
    "print(f\"Any Inf values: {np.isinf(embeddings).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing semantic similarity...\n",
      "\n",
      "Sample similarity matrix:\n",
      "Questions:\n",
      "0: At any time during this school year did you attend a public or private school in...\n",
      "1: 20. Has this happened during the past 12 months, that is from [AUTOFILL DATE 1st...\n",
      "2: ACTION_SECURITY Have you taken self-defensive actions or other security measures...\n",
      "3: How often does this child’s health insurance offer benefits or cover services th...\n",
      "4: Has a doctor or other health care provider EVER told you that this child has Aut...\n",
      "\n",
      "Cosine similarities:\n",
      "[[1.    0.993 0.993 0.994 0.993]\n",
      " [0.993 1.    0.993 0.991 0.992]\n",
      " [0.993 0.993 1.    0.993 0.992]\n",
      " [0.994 0.991 0.993 1.    0.994]\n",
      " [0.993 0.992 0.992 0.994 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Test similarity between a few questions\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\nTesting semantic similarity...\")\n",
    "\n",
    "# Pick a few sample questions\n",
    "sample_indices = [0, 100, 200, 300, 400]\n",
    "sample_texts = [question_texts[i][:80] for i in sample_indices]\n",
    "sample_embeddings = embeddings[sample_indices]\n",
    "\n",
    "# Compute pairwise similarities\n",
    "similarities = cosine_similarity(sample_embeddings)\n",
    "\n",
    "print(\"\\nSample similarity matrix:\")\n",
    "print(\"Questions:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"{i}: {text}...\")\n",
    "print(\"\\nCosine similarities:\")\n",
    "print(similarities.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embeddings...\n",
      "  ✓ Embeddings saved to: ../data/processed/embeddings/question_embeddings.npy\n",
      "  ✓ Question mapping saved to: ../data/processed/embeddings/question_id_mapping.csv\n",
      "  ✓ Pickle file saved to: ../data/processed/embeddings/embeddings_with_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving embeddings...\")\n",
    "\n",
    "# Create embeddings directory if it doesn't exist\n",
    "output_dir = Path('../data/processed/embeddings')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save embeddings as numpy array\n",
    "embeddings_path = output_dir / 'question_embeddings.npy'\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"  ✓ Embeddings saved to: {embeddings_path}\")\n",
    "\n",
    "# Save question ID mapping\n",
    "mapping_df = unique_questions[['question_id', 'question_text']].copy()\n",
    "mapping_df['embedding_index'] = range(len(mapping_df))\n",
    "mapping_path = output_dir / 'question_id_mapping.csv'\n",
    "mapping_df.to_csv(mapping_path, index=False)\n",
    "print(f\"  ✓ Question mapping saved to: {mapping_path}\")\n",
    "\n",
    "# Also save as pickle for convenience\n",
    "embedding_dict = {\n",
    "    'embeddings': embeddings,\n",
    "    'question_ids': unique_questions['question_id'].values,\n",
    "    'question_texts': unique_questions['question_text'].values,\n",
    "    'model': 'roberta-large',\n",
    "    'embedding_dim': embeddings.shape[1],\n",
    "    'num_questions': len(embeddings)\n",
    "}\n",
    "pickle_path = output_dir / 'embeddings_with_metadata.pkl'\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(embedding_dict, f)\n",
    "print(f\"  ✓ Pickle file saved to: {pickle_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EMBEDDING GENERATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Model: RoBERTa-large\n",
      "Device: cpu\n",
      "\n",
      "Questions embedded: 6,295\n",
      "Embedding dimension: 1024\n",
      "Total size: 24.6 MB\n",
      "\n",
      "Processing time: 6.4 minutes\n",
      "Average: 0.061s per question\n",
      "\n",
      "Output files:\n",
      "  - ../data/processed/embeddings/question_embeddings.npy\n",
      "  - ../data/processed/embeddings/question_id_mapping.csv\n",
      "  - ../data/processed/embeddings/embeddings_with_metadata.pkl\n",
      "\n",
      "Next step: Clustering analysis (notebook 03)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EMBEDDING GENERATION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nModel: RoBERTa-large\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"\\nQuestions embedded: {len(embeddings):,}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"Total size: {embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"\\nProcessing time: {embedding_time/60:.1f} minutes\")\n",
    "print(f\"Average: {embedding_time/len(embeddings):.3f}s per question\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {embeddings_path}\")\n",
    "print(f\"  - {mapping_path}\")\n",
    "print(f\"  - {pickle_path}\")\n",
    "print(f\"\\nNext step: Clustering analysis (notebook 03)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (survey-mapper)",
   "language": "python",
   "name": "survey-mapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
